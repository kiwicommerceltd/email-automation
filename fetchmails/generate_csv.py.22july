from google.cloud import documentai_v1 as documentai
from google.api_core.client_options import ClientOptions
from datetime import datetime
import csv
import re
import requests
import json
import mimetypes
import os
from django.conf import settings


# Set these variables
project_id = "order-processing-gen-ai"
location = "us"
processor_id = "952176022bd2d10"
file_path = "email_pdfs/test.pdf"
output_csv = "output_csv/output.csv"

# Create Document AI client
client_options = ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
client = documentai.DocumentProcessorServiceClient(client_options=client_options)

name = f"projects/{project_id}/locations/{location}/processors/{processor_id}"


def process_document(file_path):
    # Detect MIME type from file extension
    mime_type, _ = mimetypes.guess_type(file_path)
    
    if mime_type not in ["application/pdf", "image/png", "image/jpeg"]:
        raise ValueError(f"Unsupported file type: {mime_type}")

    with open(file_path, "rb") as file:
        file_content = file.read()

    document = {
        "content": file_content,
        "mime_type": mime_type
    }

    request = {"name": name, "raw_document": document}
    result = client.process_document(request=request)
    return result.document

# function to get the email id 
def extract_email_from_text(text):
    """Extract email address from document text using regex"""
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    matches = re.findall(email_pattern, text)
    return matches[0] if matches else ""  # default fallback


# function to get the order date and delivery data 
def extract_dates_from_text(text):
    """Extract PO date and delivery date from document text with flexible matching"""
    date_info = {
        "po_number": "",
        "po_date": "",
        "delivery_date": ""
    }
    
    # PO Number patterns
    po_number_patterns = [
        r'(?:PO|Purchase Order|Order)\s*(?:No|Number|#)?\s*[:]?\s*(\d+)',
        r'\bPO\b\s*(\d+)',
        r'\b(?:PO|Order)\s*(\d+)'
    ]
    
    # Improved date patterns with better context awareness
    date_patterns = [
        # Order Date patterns (more specific to avoid confusion)
         (r'PO\s*Date\s*[:]?\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})', "po_date"),

        (r'Order\s*Date\s*[:]?\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})', "po_date"),
        (r'Date\s*[:]?\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})(?=\s*Order)', "po_date"),

        (r'Delivery\s*Date\s*[:]?\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})', "delivery_date"),
        (r'Delivery\s*on\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})', "delivery_date"),
        (r'Deliver\s*by\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})', "delivery_date"),
    ]
    
    # Extract PO Number
    for pattern in po_number_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            date_info["po_number"] = match.group(1).strip()
            break
    
    # Extract all dates with context
    found_dates = []
    for pattern, date_type in date_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            found_dates.append({
                "date": match.group(1).replace(".", "/"), 
                "type": date_type,
                "position": match.start()
            })
    

    po_dates = [d for d in found_dates if d["type"] == "po_date"]
    delivery_dates = [d for d in found_dates if d["type"] == "delivery_date"]
    
    if po_dates:
        date_info["po_date"] = normalize_date(po_dates[0]["date"])
    if delivery_dates:
        date_info["delivery_date"] = normalize_date(delivery_dates[0]["date"])
    
    print(f"📅 Extracted Dates: {date_info}")
    return date_info

def normalize_date(raw_date):
    """Normalize date format to DD/MM/YYYY"""
    for fmt in ("%d/%m/%Y", "%d-%m-%Y", "%m/%d/%Y", "%Y-%m-%d"):
        try:
            date_obj = datetime.strptime(raw_date, fmt)
            return date_obj.strftime("%d/%m/%Y")
        except ValueError:
            continue
    return raw_date  # Return original if all parsing fails



def extract_data_to_csv(document, grouped_data, validated_address, output_csv):
    lines = []
    document_text = document.text

    extracted_email = extract_email_from_text(document_text)
    date_info = extract_dates_from_text(document_text)
    
    print("address data")
    print(validated_address)
    # Get grouped data
    descriptions = grouped_data.get("description", [])
    quantities = grouped_data.get("quantity", grouped_data.get("order_quantity", []))
    prices = grouped_data.get("price", [])
    product_codes = grouped_data.get("product_code", [])
    supplier_codes = grouped_data.get("supplier_code", [])
    po_no = grouped_data.get("po_no", [])
    po_number_value = po_no[0] if isinstance(po_no, list) and po_no else date_info.get("po_number", "")


    # Create LINE items
    for i in range(max(len(descriptions), len(quantities), len(prices), len(product_codes))):
        description = descriptions[i] if i < len(descriptions) else ""
        quantity = quantities[i] if i < len(quantities) else ""
        unit_price = prices[i] if i < len(prices) else ""
        product_code_full = product_codes[i] if i < len(product_codes) else ""
        supplier_code = supplier_codes[i] if i < len(supplier_codes) else ""

        # Initialize short code
        product_code_short = ""

        # Determine short code
        if product_code_full:
            if '-' in product_code_full:
                product_code_short = product_code_full.split('-')[0]
            elif supplier_code and product_code_full.isdigit():
                product_code_short = supplier_code
            else:
                product_code_short = product_code_full

        # Create CSV line
        line = [
            "LINE", str(i+1), product_code_short, "", quantity, "", product_code_full,
            "", "", unit_price, description
        ] + [""]*13 + [
            date_info["po_date"], quantity, "EA"
        ] + [""]*68
        
        lines.append(line)

        # Debug print
        print(f"\nItem {i+1}:")
        print(f"  Full Code: {product_code_full}")
        print(f"  Short Code: {product_code_short}")
        print(f"  Supplier Code: {supplier_code}")
        print(f"  Description: {description}")


    # Generate CSV
    head = [
        "HEAD", "76001", "",
        validated_address.get("v", ""),
        po_number_value,
        date_info.get("po_date", ""),
        date_info.get("delivery_date", ""),
        "", "", "", "", "",
        extracted_email, "", "", "",
        validated_address.get("q", ""),
        validated_address.get("r", ""),
        "", validated_address.get("t", ""),
        validated_address.get("u", ""),
        validated_address.get("v", ""),
        "76001",
        validated_address.get("v", ""),
        "", "", "", "", "FINAL",
        date_info.get("po_date", ""),
        "", "", "", "", "", "", "", "", "", "",
        extracted_email
    ] + [""] * 70


    
    recon = ["RECON", "HPC", "76001", po_number_value, str(len(lines))] + [""] * 95
    
    with open(output_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(head)
        writer.writerows(lines)
        writer.writerow(recon)

    print(f"\n✅ CSV saved as {output_csv} with {len(lines)} items")


def get_text(layout, document):
    """Concatenate text segments from layout."""
    response = ""
    for segment in layout.text_anchor.text_segments:
        start_index = int(segment.start_index) if segment.start_index else 0
        end_index = int(segment.end_index)
        response += document.text[start_index:end_index]
    return response.strip()


# to process to address
def validate_address(address_lines, region_code="GB", api_key="YOUR_API_KEY"):
    if not address_lines or not isinstance(address_lines, list) or not any(address_lines):
        print("Skipping validation: address_lines missing or invalid.")
        return {} 

    url = f"https://addressvalidation.googleapis.com/v1:validateAddress?key={api_key}"
    
    payload = {
        "address": {
            "regionCode": region_code,
            "addressLines": address_lines
        }
    }
    headers = {
        "Content-Type": "application/json"
    }
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    data = response.json()

    if "error" in data:
        print("❌ API Error:", data["error"])
        return None

    components = {}
    try:
        for comp in data["result"]["address"]["addressComponents"]:
            components[comp["componentType"]] = comp["componentName"]["text"]
    except Exception as e:
        print("⚠️ Parsing error:", e)

    # --- CSV field mapping logic ---
    raw_first_line = address_lines[0] if address_lines else ""

    # Fallback if point_of_interest is missing or partial
    poi = components.get("point_of_interest", "")
    if poi and len(poi.split()) >= 2:
        q = poi
    else:
        q = raw_first_line

    # r field: Combine subpremise + premise + locality + route
    r_parts = []
    for key in ["subpremise", "premise", "locality", "route"]:
        val = components.get(key)
        if val and val not in q:
            r_parts.append(val)
    r = " ".join(r_parts).strip()

    # Other fields
    s = ""  # Not needed for now, reserved
    t = components.get("postal_town", "")
    u = components.get("administrative_area_level_2", "")
    v = components.get("postal_code", "")

    return {
        "formattedAddress": data["result"]["address"].get("formattedAddress", ""),
        "components": components,
        "verdict": data["result"]["verdict"],
        "q": q,
        "r": r,
        "s": s,
        "t": t,
        "u": u,
        "v": v
    }


# to extract table data using custom processor 
def run_custom_processor_and_print_output(file_path):
    project_id = "order-processing-gen-ai"
    location = "us"
    custom_processor_id = "eaa9b1b010772675"
    version_id = "pretrained-foundation-model-v1.3-2024-08-31"
    client = documentai.DocumentProcessorServiceClient(
        client_options=ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
    )

    # ✅ Use the versioned processor path here
    processor_name = client.processor_version_path(
        project=project_id,
        location=location,
        processor=custom_processor_id,
        processor_version=version_id,
    )

    # ✅ Detect MIME type from file extension
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type not in ["application/pdf", "image/png", "image/jpeg"]:
        raise ValueError(f"Unsupported file type: {mime_type}")
    
    with open(file_path, "rb") as file:
        file_content = file.read()

    document = {
        "content": file_content,
        "mime_type": mime_type
    }

    request = {"name": processor_name, "raw_document": document}
    result = client.process_document(request=request)
    doc = result.document


    print("\n==============================")
    print("📄 Output from Custom Processor")
    print("==============================")

    # Initialize dictionaries to store grouped data
    grouped_data = {}
    confidence_data = {}

    # Process entities and group by type
    for entity in doc.entities:
        entity_type = entity.type_.lower()
        value = entity.mention_text.strip()
        confidence = round(entity.confidence * 100, 2)
        
        # Initialize lists for new entity types
        if entity_type not in grouped_data:
            grouped_data[entity_type] = []
            confidence_data[f"{entity_type}_confidence"] = []
        
        # Add values to appropriate lists
        grouped_data[entity_type].append(value)
        confidence_data[f"{entity_type}_confidence"].append(confidence)

    # Print grouped data in requested format
    print("\n📊 Grouped Data:")
    for entity_type, values in grouped_data.items():
        print(f"{entity_type}{values}")
    
    address_data = validate_address(list(set(grouped_data.get("deliver_address", []))), region_code="GB", api_key="AIzaSyA3rGJbR6g9T9IuebyKgU9rwYdRyIYKgDI")
    print(json.dumps(address_data, indent=2))
    print("not addrss data returned")

    return {
        "grouped_data": grouped_data,
        "raw_document": doc,
        "validated_address": address_data
    }

def extract_table_columns_from_documentai(file_path):
    from google.cloud import documentai_v1 as documentai
    from google.api_core.client_options import ClientOptions
    import mimetypes
    import json
    import os
    import sys
    import django
    from tabulate import tabulate

    # Setup Django
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    sys.path.insert(0, PROJECT_ROOT)
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'emailorders.settings')
    django.setup()

    from fetchmails.models import Customer

    customer_id = 2
    try:
        customer = Customer.objects.get(id=customer_id)
        customer_keys = list(customer.extra_data.keys())
        print("🧾 Mapping Keys from Customer.extra_data:", customer_keys)
    except Customer.DoesNotExist:
        print(f"No customer found with ID {customer_id}")
        return []

    project_id = "order-processing-gen-ai"
    location = "us"
    processor_id = "952176022bd2d10"

    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type not in ["application/pdf", "image/png", "image/jpeg"]:
        raise ValueError(f"Unsupported file type: {mime_type}")

    client = documentai.DocumentProcessorServiceClient(
        client_options=ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
    )

    name = f"projects/{project_id}/locations/{location}/processors/{processor_id}"

    with open(file_path, "rb") as f:
        file_content = f.read()

    document = {"content": file_content, "mime_type": mime_type}
    request = {"name": name, "raw_document": document}
    result = client.process_document(request=request)
    doc = result.document

    def extract_text(layout):
        text = ""
        for segment in layout.text_anchor.text_segments:
            start = int(segment.start_index) if segment.start_index else 0
            end = int(segment.end_index)
            text += doc.text[start:end]
        return text.strip()

    for page in doc.pages:
        for table_index, table in enumerate(page.tables, start=1):
            headers = []
            for header_row in table.header_rows:
                headers = [extract_text(cell.layout) for cell in header_row.cells]
                break  # Only use first header row

            print(f"\n📌 Extracted Headers from Table {table_index}:", headers)

            rows = []
            for row in table.body_rows:
                row_data = [extract_text(cell.layout) for cell in row.cells]
                row_data += [''] * (len(headers) - len(row_data))  # pad if needed
                rows.append(dict(zip(headers, row_data)))

            # Case-insensitive header mapping
            lower_headers_map = {h.lower(): h for h in headers}
            mapped_headers = []
            for key in customer_keys:
                key_lower = key.lower()
                if key_lower in lower_headers_map:
                    mapped_headers.append(lower_headers_map[key_lower])

            if mapped_headers:
                mapped_table = [{k: row.get(k, "") for k in mapped_headers} for row in rows]
                non_empty_rows = [row for row in mapped_table if any(row.values())]

                if non_empty_rows:
                    print(f"\n🎯 Mapped Table from Table {table_index} (Case-Insensitive Matching):")
                    print(tabulate(non_empty_rows, headers="keys", tablefmt="grid"))
                    return non_empty_rows

    print("⚠️ No mapped tables with data found.")
    return []


   
if __name__ == "__main__":
    # First process with the standard processor
    print("hiiii")
    doc = process_document(file_path)
    
    # mapped_result = extract_table_columns_from_documentai(file_path)
    # Then run the custom processor and get grouped data
    custom_result = run_custom_processor_and_print_output(file_path)
    
    # # Extract CSV using BOTH the original document and grouped data
    extract_data_to_csv(doc, custom_result["grouped_data"], custom_result["validated_address"], output_csv)

def run_document_ai_pipeline(file_path, output_csv):
    # 1. Process using default processor
    doc = process_document(file_path)

    # 2. Process using custom processor to get structured data
    custom_result = run_custom_processor_and_print_output(file_path)

    # 3. Generate CSV using raw document + extracted entities
    extract_data_to_csv(doc, custom_result["grouped_data"], custom_result["validated_address"], output_csv)

    print(f"\n✅ Pipeline complete for {file_path} → CSV saved to {output_csv}")
